<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BigPigPeiqi</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-12-04T01:33:20.363Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>BigPigPeiqi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Windows下tensorflow-gpu快速安装方法</title>
    <link href="http://yoursite.com/2019/09/15/Windows%E4%B8%8Btensorflow-gpu%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2019/09/15/Windows下tensorflow-gpu快速安装方法/</id>
    <published>2019-09-15T09:19:44.000Z</published>
    <updated>2019-12-04T01:33:20.363Z</updated>
    
    <content type="html"><![CDATA[<p>我第一次安装tensorflow-gpu时从网上搜索了一堆教程，跟着那些乱七八糟的教程整了半天都没整好，最后啥都没管直接<code>conda install tensorflow-gpu</code>了一下，测试发现没啥问题，就是不知道为什么网上没有这种方法的教程（或者这个方法只是我碰巧好用？），但是上次帮同学安装也用的这个方法，安装十分成功没有任何问题。</p><ul><li>PS：一定要用官方镜像源，不要设成清华啥的国内镜像，我当时用清华的镜像反正是不行，报错了。。。</li></ul><h5 id="详细步骤"><a href="#详细步骤" class="headerlink" title="详细步骤"></a>详细步骤</h5><ol><li>运行Anaconda Prompt</li><li>输入安装命令<code>conda install tensorflow-gpu</code></li><li>根据提示yes回车，然后耐心等待几个小时（大概要下载一个多G的东西，我用校园网峰值速度没超过1MB/s,毕竟是官方的镜像速度上不去）</li><li>更新你的显卡驱动到最新版</li></ol><p>好了，大功告成！就是这么简单！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我第一次安装tensorflow-gpu时从网上搜索了一堆教程，跟着那些乱七八糟的教程整了半天都没整好，最后啥都没管直接&lt;code&gt;conda install tensorflow-gpu&lt;/code&gt;了一下，测试发现没啥问题，就是不知道为什么网上没有这种方法的教程（或者这
      
    
    </summary>
    
      <category term="教程" scheme="http://yoursite.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="anaconda" scheme="http://yoursite.com/tags/anaconda/"/>
    
      <category term="安装" scheme="http://yoursite.com/tags/%E5%AE%89%E8%A3%85/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>基于LSTM对案件罪名进行预测（都是干货）</title>
    <link href="http://yoursite.com/2019/08/09/%E5%9F%BA%E4%BA%8ELSTM%E5%AF%B9%E6%A1%88%E4%BB%B6%E7%BD%AA%E5%90%8D%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/08/09/基于LSTM对案件罪名进行预测/</id>
    <published>2019-08-09T08:01:00.000Z</published>
    <updated>2019-09-08T09:15:23.772Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于本文"><a href="#关于本文" class="headerlink" title="关于本文"></a>关于本文</h2><p>这是我们学校一次大数据比赛的题目，我第一次参加这类比赛，作为一个大二的菜鸡，感觉在比赛过程还是有很多不足，欢迎大家对本文的不足之处提出自己的看法。  </p><h3 id="原题目如下"><a href="#原题目如下" class="headerlink" title="原题目如下"></a>原题目如下</h3><p>本题使用的数据集我放在了学校云盘里，有需要的自取：<a href="http://pan.dlut.edu.cn/share?id=st3u31s16ugh" target="_blank" rel="noopener">链接</a></p><h5 id="基于大数据的智能司法量刑"><a href="#基于大数据的智能司法量刑" class="headerlink" title="基于大数据的智能司法量刑"></a>基于大数据的智能司法量刑</h5><p>大数据技术和人工智能技术正在改变着我们的日常工作和生活，越来越多的行业与人工智能相碰撞，产生新的问题与挑战。智能教育、智能医疗、智能司法等都成为学者关注的方向。智能量刑问题旨在通过机器学习技术对案件进行分析，对案情相关的法律条文与犯罪嫌疑人罪名等信息进行预测。在司法公正、辅助办案、法律援助等方面有着重要应用。</p><p>本题目为嫌疑人罪名预测。根据案情事实，针对嫌疑人罪名进行预测分类。本任务选取案情类型较多的TOP30作为罪名空间。参赛选手需对罪名进行预测，每个案情事实可能对应多项罪名。</p><ul><li>数据描述：<br>数据集共包括50万条案件案情事实数据，数据格式为csv格式。以训练集数据为例，各字段含义如下：</li></ul><table><thead><tr><th>字段</th><th>含义</th><th>格式</th></tr></thead><tbody><tr><td>ids</td><td>案件ID信息，由案件正文内容计算hash得到</td><td>str</td></tr><tr><td>fact</td><td>案件对应的案情事实</td><td>str</td></tr><tr><td>criminal</td><td>需要进行罪名预测的目标嫌疑人</td><td>str</td></tr><tr><td>accusation</td><td>指控信息，及嫌疑人涉及的罪名信息</td><td>str</td></tr><tr><td>articles</td><td>嫌疑人涉及到的法律条文编号</td><td>str</td></tr></tbody></table><ul><li>其中，具有多个类别的数据通过英文分号“;”进行区分。测试集数据与之类似，需要参赛选手对accusation进行预测。</li><li>除案情数据外，针对法律条文数据给出编号对应的法律条文，条文根据《中华人民共和国刑法》整理，格式为csv格式，分隔符为英文逗号“,”。此部分数据作为辅助数据，选手可根据实际情况选用。各字段含义如下：</li></ul><table><thead><tr><th>字段</th><th>含义</th><th>格式</th></tr></thead><tbody><tr><td>article_ids</td><td>法律条文对应编号</td><td>str</td></tr><tr><td>article_detail</td><td>法律条文具体内容</td><td>str</td></tr></tbody></table><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ul><li>所有用到的库如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Embedding, Dense, LSTM</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure></li></ul><p>先看题目显然是个文本分析的题目，那就先按照正常套路对文本预处理就好了。</p><p>题目提供的数据里有个test.csv和一个train.csv以为他训练集和测试集都提供了，结果打开test.csv看了一眼发现这测试集里面居然是待预测数据。。。（也不知道官方怎么想的给待预测数据命名叫test。。）</p><p>train.csv中提供的数据挺干净，觉得只要用常规方法分词然后去除停用词就行了。考虑到司法案情数据里专有名词和人名比较多，用jieba分词的时候顺便给它词性分析了一波，然后根据词性把人名和专有名词都给去掉了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">factdata_raw = pd.read_csv(<span class="string">'./data/train.csv'</span>, usecols=[<span class="number">1</span>], names=[<span class="string">'Fact'</span>], encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">factlist_raw = factdata_raw.Fact.values.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结巴分词判断词性并根据词性清洗数据</span></span><br><span class="line">fact_clean1 = []</span><br><span class="line"><span class="keyword">for</span> fact <span class="keyword">in</span> factlist_raw:</span><br><span class="line">    fact_segment = []</span><br><span class="line">    fact_speechtag = pseg.cut(fact)</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> fact_speechtag:</span><br><span class="line">        <span class="keyword">if</span> w.flag != <span class="string">'nr'</span> <span class="keyword">and</span> w.flag != <span class="string">'ns'</span> <span class="keyword">and</span> w.flag != <span class="string">'x'</span> <span class="keyword">and</span> w.flag != <span class="string">'w'</span> <span class="keyword">and</span> w.flag != <span class="string">'nt'</span> <span class="keyword">and</span> w.flag != <span class="string">'nz'</span> <span class="keyword">and</span> w.flag != <span class="string">'m'</span>:</span><br><span class="line">            fact_segment.append(w.word)</span><br><span class="line">    fact_clean1.append(fact_segment)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取停用词表(停用词表网上搜一下一大堆)</span></span><br><span class="line">stopwords = pd.read_csv(<span class="string">'./data/stopwords.txt'</span>, index_col=<span class="literal">False</span>, names=[<span class="string">'stopword'</span>], encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除停用词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop_stopwords</span><span class="params">(contents, stopwords)</span>:</span></span><br><span class="line">    contents_clean = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> contents:</span><br><span class="line">        line_clean = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> line:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> stopwords:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            line_clean.append(word)</span><br><span class="line">        contents_clean.append(line_clean)</span><br><span class="line">    <span class="keyword">return</span> contents_clean</span><br><span class="line"></span><br><span class="line">stopwords = stopwords.stopword.values.tolist()</span><br><span class="line">facts_clean2 = drop_stopwords(fact_clean1, stopwords)</span><br><span class="line"><span class="comment"># 存储数据</span></span><br><span class="line">factdata_clean = pd.DataFrame(facts_clean2)</span><br><span class="line">factdata_clean.to_csv(<span class="string">'./factdata_clean.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="模型选择以及训练模型"><a href="#模型选择以及训练模型" class="headerlink" title="模型选择以及训练模型"></a>模型选择以及训练模型</h3><p>先看题，题目中是要根据所给的案情数据判断出相对应的罪名，题目说的很明确，罪名空间一共30条，很容易想到这道题目要实现的其实就是对文本进行分类，一共30个标签，一条案情对应一个或多个标签。</p><p>比赛阶段咱们尝试了很多模型，啥textCNN,textRNN,fastText都试过了，最后的结果都不是很满意，这些模型就不依次叙述了。最后我们选择了LSTM（Long Short-Term Memory）模型，结果不错，准确率能达到96%左右。</p><p>LSTM模型是CNN的改进版，可以避免常规RNN的梯度消失。。。具体关于LSTM的介绍这里就不多说了，网上搜一波就有一堆LSTM的介绍总结。</p><p>直入正题，我们的LSTM模型基于keras实现（tensorflow内置了keras不需要另外安装）。说句题外话，网上的gpu版tensorflow安装教程是真的坑，按这个gpu版的tensorflow花了了半天多的时间，过两天有空我再挂一个gpu版tensorflow的安装教程。话不多说直接上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line">EMBEDDING_DIM = <span class="number">300</span>  <span class="comment"># 词向量维度</span></span><br><span class="line">MAX_SEQUENCE_LENGTH = <span class="number">264</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">f = open(<span class="string">'./factdata_clean.csv'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">csvreader = csv.reader(f)</span><br><span class="line">factlist_clean = list(csvreader)</span><br><span class="line"><span class="keyword">del</span> factlist_clean[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> list <span class="keyword">in</span> factlist_clean:</span><br><span class="line">    <span class="keyword">while</span> <span class="string">''</span> <span class="keyword">in</span> list:</span><br><span class="line">        list.remove(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">factdata = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> factlist_clean:</span><br><span class="line">    new_txt = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            new_txt.append(word_index[word])  <span class="comment"># 把句子中的 词语转化为index</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            new_txt.append(<span class="number">0</span>)</span><br><span class="line">    factdata.append(new_txt)</span><br><span class="line"></span><br><span class="line">factdata = keras.preprocessing.sequence.pad_sequences(factdata, maxlen = MAX_SEQUENCE_LENGTH)  <span class="comment"># 补零</span></span><br><span class="line">np.save(<span class="string">'./data/factdata.npy'</span>, factdata)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 案情对应的30个罪名数据向量化</span></span><br><span class="line">accusationdata_raw = pd.read_csv(<span class="string">'./data/train.csv'</span>, usecols=[<span class="number">3</span>], names=[<span class="string">'Accusation'</span>], encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">accusationdata_raw = accusationdata_raw.Accusation.values.tolist()</span><br><span class="line">accusationdata=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> str <span class="keyword">in</span> accusationdata_raw:</span><br><span class="line">    accusationdata.append(str.split(<span class="string">';'</span>))</span><br><span class="line"></span><br><span class="line">accusationlist=[]</span><br><span class="line"><span class="keyword">for</span> list <span class="keyword">in</span> accusationdata:</span><br><span class="line">    <span class="keyword">for</span> type <span class="keyword">in</span> list:</span><br><span class="line">        <span class="keyword">if</span> type <span class="keyword">not</span> <span class="keyword">in</span> accusationlist:</span><br><span class="line">            accusationlist.append(type)</span><br><span class="line"></span><br><span class="line">accusation_vec=[]</span><br><span class="line"><span class="keyword">for</span> list <span class="keyword">in</span> accusationdata:</span><br><span class="line">    each_vec=[<span class="number">0</span>] * <span class="number">30</span></span><br><span class="line">    <span class="keyword">for</span> type <span class="keyword">in</span> list:</span><br><span class="line">        n = accusationlist.index(type)</span><br><span class="line">        each_vec[n] = <span class="number">1</span></span><br><span class="line">    accusation_vec.append(each_vec)</span><br><span class="line"></span><br><span class="line">accusation_vec=np.array(accusation_vec)</span><br><span class="line">np.save(<span class="string">'accusation_vec.npy'</span>, accusation_vec)</span><br><span class="line"></span><br><span class="line">accusationlist=np.array(accusationlist)</span><br><span class="line">np.save(<span class="string">'accusationlist.npy'</span>, accusationlist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取10000条数据做验证集</span></span><br><span class="line">w2vmodel=Word2Vec.load(<span class="string">"./data/word2vec.model"</span>)</span><br><span class="line">accusationdata = np.load(<span class="string">'./data/accusation_vec.npy'</span>)</span><br><span class="line">factdata = np.load(<span class="string">'./data/factdata.npy'</span>)</span><br><span class="line"></span><br><span class="line">fact_verify = []</span><br><span class="line">fact_train = []</span><br><span class="line"></span><br><span class="line">accu_verify = np.zeros((<span class="number">10213</span>, <span class="number">30</span>), dtype=np.int)</span><br><span class="line">accu_train = np.zeros((<span class="number">347241</span>, <span class="number">30</span>), dtype=np.int)</span><br><span class="line">n = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">357454</span>):</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">35</span> == <span class="number">0</span>:</span><br><span class="line">        fact_verify.append(factdata[i])</span><br><span class="line">        t = int(i / <span class="number">35</span>)</span><br><span class="line">        accu_verify[t] = accusationdata[i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fact_train.append(factdata[i])</span><br><span class="line">        accu_train[n] = accusationdata[i]</span><br><span class="line">        n = n + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">np.save(<span class="string">'./data/accu_verify.npy'</span>, accu_verify)</span><br><span class="line">np.save(<span class="string">'./data/fact_verify.npy'</span>, fact_verify)</span><br><span class="line">np.save(<span class="string">'./data/accu_train.npy'</span>, accu_train)</span><br><span class="line">np.save(<span class="string">'./data/fact_train.npy'</span>, fact_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对案情中的单词向量化并储存模型(直接调用Word2Vec进行，很方便)</span></span><br><span class="line">w2vmodel = Word2Vec(factlist_clean, size=<span class="number">300</span>, window=<span class="number">5</span>, min_count=<span class="number">20</span>, workers=<span class="number">4</span>) </span><br><span class="line">w2vmodel.save(<span class="string">"./data/word2vec.model"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立LSTM（基于keras）</span></span><br><span class="line"></span><br><span class="line">accu_train = np.load(<span class="string">'./data/accu_train.npy'</span>)</span><br><span class="line">fact_train = np.load(<span class="string">'./data/fact_train.npy'</span>)</span><br><span class="line"></span><br><span class="line">w2vmodel = Word2Vec.load(<span class="string">"./data/word2vec.model"</span>)</span><br><span class="line">vocab_list = [word <span class="keyword">for</span> word, Vocab <span class="keyword">in</span> w2vmodel.wv.vocab.items()]</span><br><span class="line">embeddings_matrix = np.zeros((len(vocab_list) + <span class="number">1</span>, w2vmodel.vector_size))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(vocab_list)):</span><br><span class="line">    word = vocab_list[i]  <span class="comment"># 每个词语</span></span><br><span class="line">    embeddings_matrix[i + <span class="number">1</span>] = w2vmodel.wv[word]  <span class="comment"># 词向量矩阵</span></span><br><span class="line"></span><br><span class="line">embedding_layer = Embedding(input_dim=len(embeddings_matrix),  <span class="comment"># 字典长度</span></span><br><span class="line">                            output_dim=EMBEDDING_DIM,  <span class="comment"># 词向量 长度（300）</span></span><br><span class="line">                            weights=[embeddings_matrix],  <span class="comment"># 重点：预训练的词向量系数</span></span><br><span class="line">                            input_length=MAX_SEQUENCE_LENGTH,  <span class="comment"># 每句话的 最大长度（必须padding） </span></span><br><span class="line">                            trainable=<span class="literal">False</span>,  <span class="comment"># 是否在 训练的过程中 更新词向量</span></span><br><span class="line">                            mask_zero=<span class="literal">True</span></span><br><span class="line">                            )</span><br><span class="line">lstm_layer = LSTM(<span class="number">384</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># lstm_layer = CuDNNLSTM(128, return_sequences=True)</span></span><br><span class="line">dense_layer = Dense(<span class="number">30</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(embedding_layer)</span><br><span class="line">model.add(lstm_layer)</span><br><span class="line">model.add(dense_layer)</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">fact_verify = np.load(<span class="string">'./data/fact_verify.npy'</span>)</span><br><span class="line">accu_verify = np.load(<span class="string">'./data/accu_verify.npy'</span>)</span><br><span class="line"></span><br><span class="line">model.fit(fact_train, accu_train, batch_size=<span class="number">128</span>, epochs=<span class="number">20</span>, validation_data=(fact_verify, accu_verify))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储训练模型</span></span><br><span class="line">model.save(<span class="string">'./data/model_res128_02_20.model'</span>)</span><br></pre></td></tr></table></figure><h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h3><p>预测结果就很简单了，直接调用<code>keras.models</code>提供的<code>predict_proba</code>函数就ok了，当然，函数返回值是一个30维向量，代表了该案情数据对应每个标签的可能性，也就是咱们找到最大的那个或者那几个值就是该案情对应的罪名了。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测并输出</span></span><br><span class="line">model=load_model(<span class="string">'./data/modelfinal_res128_02_20.model'</span>)</span><br><span class="line">fact_test=np.load(<span class="string">'./data/fact_test.npy'</span>)  <span class="comment"># fact_test.npy是数据预处理后的待预测案情</span></span><br><span class="line">preres=model.predict_proba(fact_test, batch_size = <span class="number">2048</span>, verbose = <span class="number">1</span>)</span><br><span class="line">pre01 = np.zeros((<span class="number">178712</span>,<span class="number">30</span>),dtype = np.int)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">178712</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        <span class="keyword">if</span> preres[i][j] &gt; <span class="number">0.5</span>:</span><br><span class="line">            pre01[i][j] = <span class="number">1</span></span><br><span class="line">accu = np.load(<span class="string">'./data/accusationlist.npy'</span>)</span><br><span class="line">maxcount = <span class="number">0</span></span><br><span class="line">maxnum = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">178712</span>):</span><br><span class="line">    check = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        <span class="keyword">if</span> pre01[i][j] == <span class="number">1</span>:</span><br><span class="line">            check = check + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> check == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">            <span class="keyword">if</span> preres[i][k] &gt; maxcount:</span><br><span class="line">                maxcount = preres[i][k]</span><br><span class="line">                maxnum = k</span><br><span class="line">        pre01[i][maxnum] = <span class="number">1</span></span><br><span class="line">predict = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">178712</span>):</span><br><span class="line">    s = <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        <span class="keyword">if</span> pre01[i][j] == <span class="number">1</span>:</span><br><span class="line">            s  = s + accu[j] + <span class="string">';'</span></span><br><span class="line">    s = s.rstrip(<span class="string">';'</span>)</span><br><span class="line">    predict.append(s)</span><br><span class="line">    </span><br><span class="line">ids = pd.read_csv(<span class="string">'./data/test.csv'</span>, usecols=[<span class="number">0</span>], names=[<span class="string">'ids'</span>], encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">ids = ids.ids.values.tolist()</span><br><span class="line">dataframe = pd.DataFrame(&#123;<span class="string">'ids'</span>:ids,<span class="string">'accusation'</span>:predict&#125;)</span><br><span class="line">dataframe.to_csv(<span class="string">"resfinal.csv"</span>,index=<span class="literal">False</span>,sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;关于本文&quot;&gt;&lt;a href=&quot;#关于本文&quot; class=&quot;headerlink&quot; title=&quot;关于本文&quot;&gt;&lt;/a&gt;关于本文&lt;/h2&gt;&lt;p&gt;这是我们学校一次大数据比赛的题目，我第一次参加这类比赛，作为一个大二的菜鸡，感觉在比赛过程还是有很多不足，欢迎大家对本文的不
      
    
    </summary>
    
      <category term="大数据分析" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
      <category term="文本分类" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
</feed>
