<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[视频分享（三）]]></title>
    <url>%2F2019%2F12%2F06%2F%E8%A7%86%E9%A2%91%E5%88%86%E4%BA%AB%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这个板块用来转载一些我所在的工作室出品的一些小视频o((&gt;ω&lt; ))o 大连理工大学2019级新生梦想采访 原视频链接https://www.bilibili.com/video/av68316517/ 梦想无论怎样模糊，总潜伏在我们心底，使我们的心境永远得不到宁静，直到这些梦想成为事实才止；像种子在地下一样，一定要萌芽滋长，伸出地面来，寻找阳光。-林语堂当你迷茫之际，希望你能不忘当初的梦想。——大工软院 你梦想启航的地方]]></content>
      <categories>
        <category>视频分享</category>
      </categories>
      <tags>
        <tag>大工软件学院创意中心</tag>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视频分享（二）]]></title>
    <url>%2F2019%2F12%2F06%2F%E8%A7%86%E9%A2%91%E5%88%86%E4%BA%AB%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这个板块用来转载一些我所在的工作室出品的一些小视频o((&gt;ω&lt; ))o 大连理工大学开发区校区2019级辅导员联合翻唱作品——重返十七岁 原视频链接https://www.bilibili.com/video/av68645888/]]></content>
      <categories>
        <category>视频分享</category>
      </categories>
      <tags>
        <tag>大工软件学院创意中心</tag>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视频分享（一）]]></title>
    <url>%2F2019%2F12%2F06%2F%E8%A7%86%E9%A2%91%E5%88%86%E4%BA%AB%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这个板块用来转载一些我所在的工作室出品的一些小视频o((&gt;ω&lt; ))o 大连理工大学开发区校区2018毕业晚会开场视频 原视频链接https://www.bilibili.com/video/av31064005/]]></content>
      <categories>
        <category>视频分享</category>
      </categories>
      <tags>
        <tag>大工软件学院创意中心</tag>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下tensorflow-gpu快速安装方法]]></title>
    <url>%2F2019%2F09%2F15%2FWindows%E4%B8%8Btensorflow-gpu%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[我第一次安装tensorflow-gpu时从网上搜索了一堆教程，跟着那些乱七八糟的教程整了半天都没整好，最后啥都没管直接conda install tensorflow-gpu了一下，测试发现没啥问题，就是不知道为什么网上没有这种方法的教程（或者这个方法只是我碰巧好用？），但是上次帮同学安装也用的这个方法，安装十分成功没有任何问题。 PS：一定要用官方镜像源，不要设成清华啥的国内镜像，我当时用清华的镜像反正是不行，报错了。。。 详细步骤 运行Anaconda Prompt 输入安装命令conda install tensorflow-gpu 根据提示yes回车，然后耐心等待几个小时（大概要下载一个多G的东西，我用校园网峰值速度没超过1MB/s,毕竟是官方的镜像速度上不去） 更新你的显卡驱动到最新版 好了，大功告成！就是这么简单！]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
        <tag>安装</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于LSTM对案件罪名进行预测（都是干货）]]></title>
    <url>%2F2019%2F08%2F09%2F%E5%9F%BA%E4%BA%8ELSTM%E5%AF%B9%E6%A1%88%E4%BB%B6%E7%BD%AA%E5%90%8D%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[关于本文这是我们学校一次大数据比赛的题目，我第一次参加这类比赛，作为一个大二的菜鸡，感觉在比赛过程还是有很多不足，欢迎大家对本文的不足之处提出自己的看法。 原题目如下本题使用的数据集我放在了学校云盘里，有需要的自取：链接 基于大数据的智能司法量刑大数据技术和人工智能技术正在改变着我们的日常工作和生活，越来越多的行业与人工智能相碰撞，产生新的问题与挑战。智能教育、智能医疗、智能司法等都成为学者关注的方向。智能量刑问题旨在通过机器学习技术对案件进行分析，对案情相关的法律条文与犯罪嫌疑人罪名等信息进行预测。在司法公正、辅助办案、法律援助等方面有着重要应用。 本题目为嫌疑人罪名预测。根据案情事实，针对嫌疑人罪名进行预测分类。本任务选取案情类型较多的TOP30作为罪名空间。参赛选手需对罪名进行预测，每个案情事实可能对应多项罪名。 数据描述：数据集共包括50万条案件案情事实数据，数据格式为csv格式。以训练集数据为例，各字段含义如下： 字段 含义 格式 ids 案件ID信息，由案件正文内容计算hash得到 str fact 案件对应的案情事实 str criminal 需要进行罪名预测的目标嫌疑人 str accusation 指控信息，及嫌疑人涉及的罪名信息 str articles 嫌疑人涉及到的法律条文编号 str 其中，具有多个类别的数据通过英文分号“;”进行区分。测试集数据与之类似，需要参赛选手对accusation进行预测。 除案情数据外，针对法律条文数据给出编号对应的法律条文，条文根据《中华人民共和国刑法》整理，格式为csv格式，分隔符为英文逗号“,”。此部分数据作为辅助数据，选手可根据实际情况选用。各字段含义如下： 字段 含义 格式 article_ids 法律条文对应编号 str article_detail 法律条文具体内容 str 数据预处理 所有用到的库如下：12345678import pandas as pdimport jieba.posseg as psegimport numpy as npimport csvfrom gensim.models import Word2Vecfrom tensorflow.keras.layers import Embedding, Dense, LSTMfrom tensorflow.keras import Sequentialfrom tensorflow import keras 先看题目显然是个文本分析的题目，那就先按照正常套路对文本预处理就好了。 题目提供的数据里有个test.csv和一个train.csv以为他训练集和测试集都提供了，结果打开test.csv看了一眼发现这测试集里面居然是待预测数据。。。（也不知道官方怎么想的给待预测数据命名叫test。。） train.csv中提供的数据挺干净，觉得只要用常规方法分词然后去除停用词就行了。考虑到司法案情数据里专有名词和人名比较多，用jieba分词的时候顺便给它词性分析了一波，然后根据词性把人名和专有名词都给去掉了。 123456789101112131415161718192021222324252627282930313233factdata_raw = pd.read_csv('./data/train.csv', usecols=[1], names=['Fact'], encoding='utf-8')factlist_raw = factdata_raw.Fact.values.tolist()# 结巴分词判断词性并根据词性清洗数据fact_clean1 = []for fact in factlist_raw: fact_segment = [] fact_speechtag = pseg.cut(fact) for w in fact_speechtag: if w.flag != 'nr' and w.flag != 'ns' and w.flag != 'x' and w.flag != 'w' and w.flag != 'nt' and w.flag != 'nz' and w.flag != 'm': fact_segment.append(w.word) fact_clean1.append(fact_segment)# 读取停用词表(停用词表网上搜一下一大堆)stopwords = pd.read_csv('./data/stopwords.txt', index_col=False, names=['stopword'], encoding='utf-8')# 去除停用词def drop_stopwords(contents, stopwords): contents_clean = [] for line in contents: line_clean = [] for word in line: if word in stopwords: continue line_clean.append(word) contents_clean.append(line_clean) return contents_cleanstopwords = stopwords.stopword.values.tolist()facts_clean2 = drop_stopwords(fact_clean1, stopwords)# 存储数据factdata_clean = pd.DataFrame(facts_clean2)factdata_clean.to_csv('./factdata_clean.csv', index=False) 模型选择以及训练模型先看题，题目中是要根据所给的案情数据判断出相对应的罪名，题目说的很明确，罪名空间一共30条，很容易想到这道题目要实现的其实就是对文本进行分类，一共30个标签，一条案情对应一个或多个标签。 比赛阶段咱们尝试了很多模型，啥textCNN,textRNN,fastText都试过了，最后的结果都不是很满意，这些模型就不依次叙述了。最后我们选择了LSTM（Long Short-Term Memory）模型，结果不错，准确率能达到96%左右。 LSTM模型是CNN的改进版，可以避免常规RNN的梯度消失。。。具体关于LSTM的介绍这里就不多说了，网上搜一波就有一堆LSTM的介绍总结。 直入正题，我们的LSTM模型基于keras实现（tensorflow内置了keras不需要另外安装）。说句题外话，网上的gpu版tensorflow安装教程是真的坑，按这个gpu版的tensorflow花了了半天多的时间，过两天有空我再挂一个gpu版tensorflow的安装教程。话不多说直接上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124EMBEDDING_DIM = 300 # 词向量维度MAX_SEQUENCE_LENGTH = 264# 读取数据f = open('./factdata_clean.csv', 'r', encoding='utf-8')csvreader = csv.reader(f)factlist_clean = list(csvreader)del factlist_clean[0]for list in factlist_clean: while '' in list: list.remove('')factdata = []for sentence in factlist_clean: new_txt = [] for word in sentence: try: new_txt.append(word_index[word]) # 把句子中的 词语转化为index except: new_txt.append(0) factdata.append(new_txt)factdata = keras.preprocessing.sequence.pad_sequences(factdata, maxlen = MAX_SEQUENCE_LENGTH) # 补零np.save('./data/factdata.npy', factdata)# 案情对应的30个罪名数据向量化accusationdata_raw = pd.read_csv('./data/train.csv', usecols=[3], names=['Accusation'], encoding='utf-8')accusationdata_raw = accusationdata_raw.Accusation.values.tolist()accusationdata=[]for str in accusationdata_raw: accusationdata.append(str.split(';'))accusationlist=[]for list in accusationdata: for type in list: if type not in accusationlist: accusationlist.append(type)accusation_vec=[]for list in accusationdata: each_vec=[0] * 30 for type in list: n = accusationlist.index(type) each_vec[n] = 1 accusation_vec.append(each_vec)accusation_vec=np.array(accusation_vec)np.save('accusation_vec.npy', accusation_vec)accusationlist=np.array(accusationlist)np.save('accusationlist.npy', accusationlist)# 提取10000条数据做验证集w2vmodel=Word2Vec.load("./data/word2vec.model")accusationdata = np.load('./data/accusation_vec.npy')factdata = np.load('./data/factdata.npy')fact_verify = []fact_train = []accu_verify = np.zeros((10213, 30), dtype=np.int)accu_train = np.zeros((347241, 30), dtype=np.int)n = 0for i in range(357454): if i % 35 == 0: fact_verify.append(factdata[i]) t = int(i / 35) accu_verify[t] = accusationdata[i] else: fact_train.append(factdata[i]) accu_train[n] = accusationdata[i] n = n + 1np.save('./data/accu_verify.npy', accu_verify)np.save('./data/fact_verify.npy', fact_verify)np.save('./data/accu_train.npy', accu_train)np.save('./data/fact_train.npy', fact_train)# 对案情中的单词向量化并储存模型(直接调用Word2Vec进行，很方便)w2vmodel = Word2Vec(factlist_clean, size=300, window=5, min_count=20, workers=4) w2vmodel.save("./data/word2vec.model")# 建立LSTM（基于keras）accu_train = np.load('./data/accu_train.npy')fact_train = np.load('./data/fact_train.npy')w2vmodel = Word2Vec.load("./data/word2vec.model")vocab_list = [word for word, Vocab in w2vmodel.wv.vocab.items()]embeddings_matrix = np.zeros((len(vocab_list) + 1, w2vmodel.vector_size))for i in range(len(vocab_list)): word = vocab_list[i] # 每个词语 embeddings_matrix[i + 1] = w2vmodel.wv[word] # 词向量矩阵embedding_layer = Embedding(input_dim=len(embeddings_matrix), # 字典长度 output_dim=EMBEDDING_DIM, # 词向量 长度（300） weights=[embeddings_matrix], # 重点：预训练的词向量系数 input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） trainable=False, # 是否在 训练的过程中 更新词向量 mask_zero=True )lstm_layer = LSTM(384, dropout=0.2, recurrent_dropout=0.2)# lstm_layer = CuDNNLSTM(128, return_sequences=True)dense_layer = Dense(30, activation='sigmoid')model = Sequential()model.add(embedding_layer)model.add(lstm_layer)model.add(dense_layer)model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])fact_verify = np.load('./data/fact_verify.npy')accu_verify = np.load('./data/accu_verify.npy')model.fit(fact_train, accu_train, batch_size=128, epochs=20, validation_data=(fact_verify, accu_verify))# 存储训练模型model.save('./data/model_res128_02_20.model') 预测结果预测结果就很简单了，直接调用keras.models提供的predict_proba函数就ok了，当然，函数返回值是一个30维向量，代表了该案情数据对应每个标签的可能性，也就是咱们找到最大的那个或者那几个值就是该案情对应的罪名了。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940from tensorflow.keras.models import load_modelimport numpy as npimport pandas as pd## 预测并输出model=load_model('./data/modelfinal_res128_02_20.model')fact_test=np.load('./data/fact_test.npy') # fact_test.npy是数据预处理后的待预测案情preres=model.predict_proba(fact_test, batch_size = 2048, verbose = 1)pre01 = np.zeros((178712,30),dtype = np.int)for i in range(178712): for j in range(30): if preres[i][j] &gt; 0.5: pre01[i][j] = 1accu = np.load('./data/accusationlist.npy')maxcount = 0maxnum = 0for i in range(178712): check = 0 for j in range(30): if pre01[i][j] == 1: check = check + 1 if check == 0: for k in range(30): if preres[i][k] &gt; maxcount: maxcount = preres[i][k] maxnum = k pre01[i][maxnum] = 1predict = []for i in range(178712): s = "" for j in range(30): if pre01[i][j] == 1: s = s + accu[j] + ';' s = s.rstrip(';') predict.append(s) ids = pd.read_csv('./data/test.csv', usecols=[0], names=['ids'], encoding='utf-8')ids = ids.ids.values.tolist()dataframe = pd.DataFrame(&#123;'ids':ids,'accusation':predict&#125;)dataframe.to_csv("resfinal.csv",index=False,sep=',')]]></content>
      <categories>
        <category>大数据分析</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>深度学习</tag>
        <tag>LSTM</tag>
        <tag>文本分类</tag>
      </tags>
  </entry>
</search>
